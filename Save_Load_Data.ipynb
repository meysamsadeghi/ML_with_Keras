{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to save and load data in Python\n",
    "In this notebook I will cover the followings:\n",
    "- Save and load files using HDF5 (using h5py package)\n",
    "- Save and load files using pickle\n",
    "- Save and load files from MATLAB\n",
    "- Save and load files using HDF5 (using Pandas package) # to be added\n",
    "\n",
    "To continue we need to create some data, e.g., data1 and data2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10586635 0.53926726 0.88506764]\n",
      " [0.92514506 0.43486768 0.111493  ]]\n"
     ]
    }
   ],
   "source": [
    "data1 = np.random.random((10,10))\n",
    "data2 = np.random.random((100,20))\n",
    "print(data2[20:22,3:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5\n",
    "There are three main types of items in HDF5 files:\n",
    "- File\n",
    "- Group\n",
    "- Dataset\n",
    "Their name is used as access key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF Files: How to Create HDF file and how to Read from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of datasets in this file: ['d1', 'd2']\n",
      "[[0.10586635 0.53926726 0.88506764]\n",
      " [0.92514506 0.43486768 0.111493  ]]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# How to create and save hdf files\n",
    "with h5py.File('/home/meysam/github/ML_with_Keras/SaveLoad/HDF_filename.h5','w') as h: # replace HDF_filename.h5 with your desired file name \n",
    "    h.create_dataset('d1',data=data1) # inside prantesis first a name and then data is passed\n",
    "    h.create_dataset('d2',data=data2)\n",
    "    \n",
    "# how to read from the hdf files\n",
    "with h5py.File('/home/meysam/github/ML_with_Keras/SaveLoad/HDF_filename.h5','r') as h: # healthy practice:use contet manager\n",
    "    ls = list(h.keys()) # this gives the list of names in the dataset\n",
    "    print('List of datasets in this file:', ls)\n",
    "    da1 = h.get('d1')\n",
    "    loaded_data1 = np.array(da1)\n",
    "    da2 = h.get('d2')\n",
    "    loaded_data2 = np.asarray(da2)\n",
    "    \n",
    "# sanity check\n",
    "print(loaded_data2[20:22,3:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF Groups: How to Create HDF Groups and how to Read from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56184845, 0.75124976]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix1 = np.random.random((20,20))\n",
    "matrix2 = np.random.random((20,20))\n",
    "matrix3 = np.random.random((20,20))\n",
    "matrix4 = np.random.random((20,20))\n",
    "matrix3[4:5,6:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to create groups and subgroups\n",
    "with h5py.File('/home/meysam/github/ML_with_Keras/SaveLoad/HDF_groups.h5','w') as h:\n",
    "    G1 = h.create_group('Group1')\n",
    "    G1.create_dataset('dataset1',data=matrix1)\n",
    "    G1.create_dataset('dataset4',data=matrix4)\n",
    "    \n",
    "    G2 = h.create_group('Group2/subGroup1')\n",
    "    G2.create_dataset('dataset3',data=matrix3)\n",
    "\n",
    "    G2_2 = h.create_group('Group2/subGroup2')\n",
    "    G2_2.create_dataset('dataset2',data=matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of datasets in the base dictionary: \n",
      " [('Group1', <HDF5 group \"/Group1\" (2 members)>), ('Group2', <HDF5 group \"/Group2\" (2 members)>)]\n",
      "Items in group 1:\n",
      " [('dataset1', <HDF5 dataset \"dataset1\": shape (20, 20), type \"<f8\">), ('dataset4', <HDF5 dataset \"dataset4\": shape (20, 20), type \"<f8\">)]\n",
      "Items in group 2:\n",
      " [('subGroup1', <HDF5 group \"/Group2/subGroup1\" (1 members)>), ('subGroup2', <HDF5 group \"/Group2/subGroup2\" (1 members)>)]\n",
      "Items in subgroup 1 of group 2:\n",
      " [('dataset3', <HDF5 dataset \"dataset3\": shape (20, 20), type \"<f8\">)]\n"
     ]
    }
   ],
   "source": [
    "# How to Read groups and subgroups, Also how to check their members\n",
    "with h5py.File('/home/meysam/github/ML_with_Keras/SaveLoad/HDF_groups.h5','r') as h:\n",
    "    ls_items = list(h.items()) #note the difference h.items() not h.keys()\n",
    "    print('List of datasets in the base dictionary: \\n', ls_items)\n",
    "    G1 = h.get('Group1')\n",
    "    G1_items = list(G1.items())\n",
    "    print('Items in group 1:\\n',G1_items)\n",
    "    \n",
    "    G2 = h.get('Group2')\n",
    "    G2_items = list(G2.items())\n",
    "    print('Items in group 2:\\n',G2_items)\n",
    "    \n",
    "    # read the dataset in subgroup 1 of group 2\n",
    "    G2_1 = G2.get('/Group2/subGroup1') # the address is obtained from list(G2.items())\n",
    "    G2_1_items = list(G2_1.items())\n",
    "    print('Items in subgroup 1 of group 2:\\n',G2_1_items)\n",
    "    mat3 = np.array(G2_1.get('dataset3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56184845, 0.75124976]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat3[4:5,6:8] # sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can compress the datasets, or give them attributes, etc. More details __[here](https://www.youtube.com/watch?v=beat9RO0-x4&index=8&list=PLea0WJq13cnB_ORdGzEkPlZEN20TSt6Lx)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pickle\n",
    "### How to save and also load files in pickle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10586635 0.53926726 0.88506764]\n",
      " [0.92514506 0.43486768 0.111493  ]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Save files with pickle\n",
    "# with open('name_by_which_file_will_be_saved','wb') as pw:\n",
    "#     pw.dump([var1,var2, ...(vars to be saved)],f)\n",
    "with open('/home/meysam/github/ML_with_Keras/SaveLoad/Pickle_filename','wb') as pw:\n",
    "    pickle.dump([data1, data2],pw)\n",
    "\n",
    "    \n",
    "# Load files with pickle\n",
    "# with open('name_of_file_to_be_loaded','rb') as pr:\n",
    "#     [var1,var2, ...(vars to be load)] = pr.load(pr)\n",
    "with open('/home/meysam/github/ML_with_Keras/SaveLoad/Pickle_filename','rb') as pr:\n",
    "    data1_pickle, data2_pickle = pickle.load(pr)\n",
    "\n",
    "\n",
    "# Sanity check\n",
    "print(data2_pickle[20:22,3:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATLAB\n",
    "### How to read data from Matlab files, i.e., .mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "import hdf5storage      # This package works for all .mat files, especially the matlab file version is v7.3 and works for everything\n",
    "desired_name_for_loaded_file = hdf5storage.loadmat(FileName)  \n",
    "\n",
    "# Method 2\n",
    "import scipy.io as sio  # This package is less general and do not work for .mat files v7.3 \n",
    "desired_name_for_loaded_file = sio.loadmat(FileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 with pandas\n",
    "See __[here](https://www.youtube.com/watch?v=beat9RO0-x4&index=8&list=PLea0WJq13cnB_ORdGzEkPlZEN20TSt6Lx)__."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
